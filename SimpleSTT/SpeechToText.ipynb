{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpeechToText.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1mEpcMq9bAjM8i1UV1iB5qV9DIPie5DiF",
      "authorship_tag": "ABX9TyO61GeTnzbrindv5jZf2ABO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeyboarderSon/TIL/blob/main/SimpleSTT/SpeechToText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9SXxf1ySzqF"
      },
      "source": [
        "https://www.analyticsvidhya.com/blog/2019/07/learn-build-first-speech-to-text-model-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_JWb7CdS7bV"
      },
      "source": [
        "##Audio waves 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uc-2qYr243H"
      },
      "source": [
        "앞서 살펴보았듯,\r\n",
        " 녹음본의 길이가 1초 이하고 sampling rate은 너무 높았다.<br>\r\n",
        "그래서 resampling을 해보겠다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_xuNSwoVgVE"
      },
      "source": [
        "import os\r\n",
        "import librosa   #for audio processing\r\n",
        "import IPython.display as ipd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from scipy.io import wavfile #for audio processing\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M28r5k0i5Nqb",
        "outputId": "2446d756-2cec-4a65-e69b-a165f69cdaeb"
      },
      "source": [
        "from google.colab import drive\r\n",
        "ROOT=\"/content/drive\"\r\n",
        "MY_GOOGLE_DRIVE_PATH=\"MyDrive/SimpleSTT/data\"\r\n",
        "PROJECT_PATH=os.path.join(ROOT, MY_GOOGLE_DRIVE_PATH)\r\n",
        "drive.mount(ROOT)\r\n",
        "\r\n",
        "train_audio_path = PROJECT_PATH+'/train/audio'\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeN2R4RXA0Yw"
      },
      "source": [
        "labels=[\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"]\r\n",
        "\r\n",
        "\r\n",
        "all_wave=[]\r\n",
        "all_label=[]\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R-g24Kk5ggu"
      },
      "source": [
        "아래 작업이 시간이 오래 걸려서 한 라벨에 대해 처리가 끝나면 npy 파일로 저장해 내보냈다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz5g9QiZT0vz"
      },
      "source": [
        "\r\n",
        "for label in labels:\r\n",
        "  print(label)\r\n",
        "  waves=[f for f in os.listdir(train_audio_path + '/' + label) if f.endswith('.wav')]\r\n",
        "  for wav in waves:\r\n",
        "    samples, sample_rate = librosa.load(train_audio_path + '/' + label + '/' + wav, sr = 16000)\r\n",
        "    samples = librosa.resample(samples, sample_rate, 8000)\r\n",
        "    if(len(samples)==8000):\r\n",
        "      all_wave_test.append(samples)\r\n",
        "      all_label_test.append(label)\r\n",
        "\r\n",
        "\r\n",
        "  np.save(PROJECT_PATH+'/train/SaveArr/'+label,np.array(all_wave))\r\n",
        "  np.save(PROJECT_PATH+'/train/SaveArr/'+label+'_label',np.array(all_label))\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ65eszkeYVt"
      },
      "source": [
        " 위에서 저장했던 Array를 load한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I22T0FqNph3w"
      },
      "source": [
        "all_wave=[]\r\n",
        "all_label=[]\r\n",
        "NPY_PATH=PROJECT_PATH+'/train/SaveArr/'\r\n",
        "\r\n",
        "for label in  labels:\r\n",
        "  all_wave.extend(np.load(NPY_PATH+label+'.npy'))\r\n",
        "  all_label.extend(np.load(NPY_PATH+label+'_label.npy'))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQrTpMzCFq_A"
      },
      "source": [
        "### categorical 데이터를 numerical로 변환하기\r\n",
        "\r\n",
        "**LabelEncoder**\r\n",
        "을 사용하면,<br>\r\n",
        "숫자형으로 리턴될 뿐만 아니라\r\n",
        "그 숫자가 어떠한 class를 나타내는지도 알 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKc-C5Ew-iWB"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "le = LabelEncoder()\r\n",
        "y = le.fit_transform(all_label)\r\n",
        "\r\n",
        "#변환된 label classes 확인\r\n",
        "classes = list(le.classes_)\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqXVYjQnicv8"
      },
      "source": [
        "우리는 지금 **multi-classification 문제**를 다루고 있기에 interger encoded label을 one-hot vector로 변환한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTBOV7hoHeb-"
      },
      "source": [
        "from keras.utils import np_utils\r\n",
        "y = np_utils.to_categorical(y, num_classes=len(labels))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4wOS0xJjYw-"
      },
      "source": [
        "conv1d의 input array는 3 dimension이기 때문에 우리는 2D array를 3D array로 변환한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsdd993rjSZ1"
      },
      "source": [
        "all_wave = np.array(all_wave).reshape(-1, 8000, 1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esMdFqD2jtSu"
      },
      "source": [
        "train set(80%)과 validation set(20%)으로 나누기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skpSWKMfmZs0"
      },
      "source": [
        "```\r\n",
        "stratify는 계층적 데이터 추출 옵션이다.\r\n",
        "\r\n",
        "분류 모델에서 추천!!\r\n",
        "\r\n",
        "여러 층으로 분할 후 각 층별로 랜덤 데이터를 추출한다. 원래 데이터의 분포와 유사하게 데이터를 추출한다.\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwlZ_Z70j1xv"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(\r\n",
        "    np.array(all_wave), np.array(y),\r\n",
        "    stratify = y, test_size = 0.2,\r\n",
        "    random_state = 777, shuffle=True\r\n",
        ")\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSNDjFPaOZEb"
      },
      "source": [
        "shape의 가장 왼쪽 숫자가 일치하는지 한번 확인해본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVfmZ-KdN4zU",
        "outputId": "910505fa-a87f-45f5-89e9-f0c8fff7f3cb"
      },
      "source": [
        "print(np.array(x_tr).shape, np.array(y_tr).shape)\r\n",
        "print(np.array(x_val).shape, np.array(y_val).shape)\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(93660, 8000, 1) (93660, 10)\n",
            "(23415, 8000, 1) (23415, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQzBPlF7nUqs"
      },
      "source": [
        "## 이제 진짜 모델을 만들어보자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSAQtM5BK-q8"
      },
      "source": [
        "` Input `→{`Conv1D`→`Max Pooling`}(x4)→`Dense`→`Output`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVpZ984pm_ey",
        "outputId": "404ebc04-3929-457b-fb9b-0e2fbf940ab2"
      },
      "source": [
        "from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\r\n",
        "from keras.models import Model\r\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "from keras import backend as K\r\n",
        "K.clear_session()\r\n",
        "\r\n",
        "inputs = Input(shape=(8000, 1))\r\n",
        "\r\n",
        "#First Conv1D layer\r\n",
        "#(filter, kernel_size)\r\n",
        "#filter : 출력 차원의 수\r\n",
        "#kernel_size 합성곱이 적용되는 가로 길이\r\n",
        "conv = Conv1D(8, 13, padding=\"valid\", activation=\"relu\", strides=1)(inputs)\r\n",
        "# feature map의 크기를 줄이거나  주요한 특징을 뽑아내기 위해\r\n",
        "conv = MaxPooling1D(3)(conv)\r\n",
        "#input data의 30퍼센트의 노드들을 무작위로 0으로 만듦.\r\n",
        "conv = Dropout(0.3)(conv)\r\n",
        "\r\n",
        "conv = Conv1D(16, 11, padding='valid', activation = 'relu', strides=1)(conv)\r\n",
        "conv = MaxPooling1D(3)(conv)\r\n",
        "conv = Dropout(0.3)(conv)\r\n",
        "\r\n",
        "conv = Conv1D(32, 9, padding='valid', activation = 'relu', strides=1)(conv)\r\n",
        "conv = MaxPooling1D(3)(conv)\r\n",
        "conv = Dropout(0.3)(conv)\r\n",
        "\r\n",
        "conv = Conv1D(64, 7, padding='valid', activation = 'relu', strides=1)(conv)\r\n",
        "conv = MaxPooling1D(3)(conv)\r\n",
        "conv = Dropout(0.3)(conv)\r\n",
        "\r\n",
        "#Flatten layer\r\n",
        "conv = Flatten()(conv)\r\n",
        "\r\n",
        "conv = Dense(256, activation='relu')(conv)\r\n",
        "conv = Dropout(0.3)(conv)\r\n",
        "\r\n",
        "conv = Dense(128, activation='relu')(conv)\r\n",
        "conv = Dropout(0.3)(conv)\r\n",
        "\r\n",
        "outputs = Dense(len(labels), activation='softmax')(conv)\r\n",
        "\r\n",
        "model = Model(inputs, outputs)\r\n",
        "model.summary()\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 8000, 1)]         0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 7988, 8)           112       \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 2662, 8)           0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2662, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 2652, 16)          1424      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 884, 16)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 884, 16)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 876, 32)           4640      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 292, 32)           0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 292, 32)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 286, 64)           14400     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 95, 64)            0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 95, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6080)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               1556736   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,611,498\n",
            "Trainable params: 1,611,498\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMXkUITBYvTc"
      },
      "source": [
        "multi classification을 다루고 있기에 loss function을 categorical cross entropy로 정의한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njn-GR_Nj7Va"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KhZPkh1ZzYI"
      },
      "source": [
        "Early Stopping과 model checkpoints는 매 epoch를 수행하면 best model을 저장하고 accuracy에 더이상 진전이 보이지 않으면 early stopping을 수행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63-nEc5SmKBt"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001) \r\n",
        "mc = ModelCheckpoint('best_model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCTvuwB6aQEt"
      },
      "source": [
        "batch size는 32로 지정하고 training하고 \r\n",
        "evaluate한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CwTs4IrmMjl",
        "outputId": "490fe654-d15f-4573-96e2-3190fa8a49dc"
      },
      "source": [
        "history=model.fit(x_tr, y_tr ,epochs=100, callbacks=[es,mc], batch_size=32, validation_data=(x_val,y_val))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2927/2927 [==============================] - 32s 8ms/step - loss: 1.5188 - accuracy: 0.4598 - val_loss: 0.5523 - val_accuracy: 0.8234\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.82345, saving model to best_model.hdf5\n",
            "Epoch 2/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.6143 - accuracy: 0.7987 - val_loss: 0.3778 - val_accuracy: 0.8762\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.82345 to 0.87623, saving model to best_model.hdf5\n",
            "Epoch 3/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.4662 - accuracy: 0.8481 - val_loss: 0.2839 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.87623 to 0.90797, saving model to best_model.hdf5\n",
            "Epoch 4/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.3993 - accuracy: 0.8714 - val_loss: 0.2665 - val_accuracy: 0.9127\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90797 to 0.91271, saving model to best_model.hdf5\n",
            "Epoch 5/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.3522 - accuracy: 0.8848 - val_loss: 0.2280 - val_accuracy: 0.9243\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.91271 to 0.92432, saving model to best_model.hdf5\n",
            "Epoch 6/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.3187 - accuracy: 0.8946 - val_loss: 0.2232 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.92432 to 0.92859, saving model to best_model.hdf5\n",
            "Epoch 7/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.2952 - accuracy: 0.9035 - val_loss: 0.2107 - val_accuracy: 0.9307\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.92859 to 0.93069, saving model to best_model.hdf5\n",
            "Epoch 8/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.2810 - accuracy: 0.9082 - val_loss: 0.1680 - val_accuracy: 0.9471\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.93069 to 0.94713, saving model to best_model.hdf5\n",
            "Epoch 9/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.2641 - accuracy: 0.9140 - val_loss: 0.1599 - val_accuracy: 0.9514\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.94713 to 0.95136, saving model to best_model.hdf5\n",
            "Epoch 10/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.2515 - accuracy: 0.9171 - val_loss: 0.1560 - val_accuracy: 0.9512\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.95136\n",
            "Epoch 11/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.2405 - accuracy: 0.9213 - val_loss: 0.1719 - val_accuracy: 0.9435\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.95136\n",
            "Epoch 12/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.2257 - accuracy: 0.9273 - val_loss: 0.1483 - val_accuracy: 0.9504\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.95136\n",
            "Epoch 13/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.2247 - accuracy: 0.9290 - val_loss: 0.1195 - val_accuracy: 0.9629\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.95136 to 0.96289, saving model to best_model.hdf5\n",
            "Epoch 14/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.2147 - accuracy: 0.9301 - val_loss: 0.1241 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.96289\n",
            "Epoch 15/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.2058 - accuracy: 0.9338 - val_loss: 0.1168 - val_accuracy: 0.9624\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.96289\n",
            "Epoch 16/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1993 - accuracy: 0.9358 - val_loss: 0.1053 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.96289 to 0.96793, saving model to best_model.hdf5\n",
            "Epoch 17/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.2035 - accuracy: 0.9349 - val_loss: 0.1096 - val_accuracy: 0.9665\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.96793\n",
            "Epoch 18/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.2001 - accuracy: 0.9367 - val_loss: 0.1186 - val_accuracy: 0.9637\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.96793\n",
            "Epoch 19/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1885 - accuracy: 0.9384 - val_loss: 0.1050 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.96793\n",
            "Epoch 20/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1822 - accuracy: 0.9408 - val_loss: 0.0975 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.96793 to 0.96963, saving model to best_model.hdf5\n",
            "Epoch 21/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1925 - accuracy: 0.9399 - val_loss: 0.0959 - val_accuracy: 0.9705\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.96963 to 0.97049, saving model to best_model.hdf5\n",
            "Epoch 22/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1774 - accuracy: 0.9424 - val_loss: 0.1015 - val_accuracy: 0.9690\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.97049\n",
            "Epoch 23/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1838 - accuracy: 0.9423 - val_loss: 0.1009 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.97049\n",
            "Epoch 24/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1684 - accuracy: 0.9462 - val_loss: 0.0915 - val_accuracy: 0.9722\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.97049 to 0.97215, saving model to best_model.hdf5\n",
            "Epoch 25/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1673 - accuracy: 0.9461 - val_loss: 0.0946 - val_accuracy: 0.9702\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.97215\n",
            "Epoch 26/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1707 - accuracy: 0.9467 - val_loss: 0.1049 - val_accuracy: 0.9694\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.97215\n",
            "Epoch 27/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1642 - accuracy: 0.9479 - val_loss: 0.0962 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.97215\n",
            "Epoch 28/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1629 - accuracy: 0.9480 - val_loss: 0.0956 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.97215\n",
            "Epoch 29/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1597 - accuracy: 0.9496 - val_loss: 0.1020 - val_accuracy: 0.9698\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.97215\n",
            "Epoch 30/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1607 - accuracy: 0.9490 - val_loss: 0.0885 - val_accuracy: 0.9722\n",
            "\n",
            "Epoch 00030: val_accuracy improved from 0.97215 to 0.97224, saving model to best_model.hdf5\n",
            "Epoch 31/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1601 - accuracy: 0.9499 - val_loss: 0.0921 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.97224\n",
            "Epoch 32/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1557 - accuracy: 0.9512 - val_loss: 0.1053 - val_accuracy: 0.9673\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.97224\n",
            "Epoch 33/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1587 - accuracy: 0.9505 - val_loss: 0.0877 - val_accuracy: 0.9736\n",
            "\n",
            "Epoch 00033: val_accuracy improved from 0.97224 to 0.97365, saving model to best_model.hdf5\n",
            "Epoch 34/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1549 - accuracy: 0.9506 - val_loss: 0.1333 - val_accuracy: 0.9568\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.97365\n",
            "Epoch 35/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1548 - accuracy: 0.9508 - val_loss: 0.0814 - val_accuracy: 0.9749\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.97365 to 0.97493, saving model to best_model.hdf5\n",
            "Epoch 36/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1547 - accuracy: 0.9519 - val_loss: 0.0823 - val_accuracy: 0.9742\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.97493\n",
            "Epoch 37/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1485 - accuracy: 0.9542 - val_loss: 0.1015 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.97493\n",
            "Epoch 38/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1475 - accuracy: 0.9541 - val_loss: 0.0835 - val_accuracy: 0.9745\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.97493\n",
            "Epoch 39/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1555 - accuracy: 0.9509 - val_loss: 0.0736 - val_accuracy: 0.9779\n",
            "\n",
            "Epoch 00039: val_accuracy improved from 0.97493 to 0.97792, saving model to best_model.hdf5\n",
            "Epoch 40/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1440 - accuracy: 0.9552 - val_loss: 0.0730 - val_accuracy: 0.9779\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.97792\n",
            "Epoch 41/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1451 - accuracy: 0.9548 - val_loss: 0.0874 - val_accuracy: 0.9732\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.97792\n",
            "Epoch 42/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1467 - accuracy: 0.9527 - val_loss: 0.0886 - val_accuracy: 0.9745\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.97792\n",
            "Epoch 43/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1451 - accuracy: 0.9544 - val_loss: 0.0899 - val_accuracy: 0.9723\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.97792\n",
            "Epoch 44/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1441 - accuracy: 0.9543 - val_loss: 0.0926 - val_accuracy: 0.9724\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.97792\n",
            "Epoch 45/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1445 - accuracy: 0.9552 - val_loss: 0.0880 - val_accuracy: 0.9728\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.97792\n",
            "Epoch 46/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1453 - accuracy: 0.9552 - val_loss: 0.0828 - val_accuracy: 0.9752\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.97792\n",
            "Epoch 47/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1440 - accuracy: 0.9560 - val_loss: 0.0963 - val_accuracy: 0.9698\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.97792\n",
            "Epoch 48/100\n",
            "2927/2927 [==============================] - 25s 8ms/step - loss: 0.1415 - accuracy: 0.9552 - val_loss: 0.0887 - val_accuracy: 0.9731\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.97792\n",
            "Epoch 49/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1457 - accuracy: 0.9547 - val_loss: 0.0809 - val_accuracy: 0.9760\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.97792\n",
            "Epoch 50/100\n",
            "2927/2927 [==============================] - 24s 8ms/step - loss: 0.1392 - accuracy: 0.9567 - val_loss: 0.0779 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.97792\n",
            "Epoch 00050: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr_cfTlGnrBv"
      },
      "source": [
        "import json\r\n",
        "with open(PROJECT_PATH+'/train/'+'history_file.json', 'w') as f:\r\n",
        "  json.dump(history.history, f)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm80oCCFn0du",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "c469562a-b9c6-4efa-d48e-1c7eb51920ad"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.plot(history.history['loss'], label='train')\r\n",
        "plt.plot(history.history['val_loss'], label='test')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVoVL3My6la2"
      },
      "source": [
        "best model을 다시 사용가능할 수 있도록 내보냈고 이를 다시 load했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyVGGycQ1qWi"
      },
      "source": [
        "model.save(PROJECT_PATH+'/train/'+'my_model.h5')\r\n",
        "load_model('best_model.hdf5').save(PROJECT_PATH+'/train/'+'best_model.hdf5')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvqOds0ZRYzg"
      },
      "source": [
        "from keras.models import load_model\r\n",
        "model=load_model(PROJECT_PATH+'/train/'+'best_model.hdf5')\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0yqgKok61VF"
      },
      "source": [
        "## Prediction 작업을 시작해보자\r\n",
        "\r\n",
        "audio가 주어지면 text로 예측하는 함수를 만들어보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqh2K32xbIh6"
      },
      "source": [
        "def predict(audio):\r\n",
        "  prob=model.predict(audio.reshape(1,8000,1))\r\n",
        "  index=np.argmax(prob[0])\r\n",
        "  return classes[index]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilkIaUig7L_f"
      },
      "source": [
        "validation data에 관하여"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9VWvFdJy0Gp",
        "outputId": "a401a428-2687-449c-aa56-ac9ab2becddd"
      },
      "source": [
        "import random\r\n",
        "index=random.randint(0, len(x_val)-1)\r\n",
        "samples=x_val[index].ravel()\r\n",
        "print(\"Audio:\", classes[np.argmax(y_val[index])])\r\n",
        "ipd.Audio(samples, rate=8000)\r\n",
        "print(\"Text : \", predict(samples))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Audio: right\n",
            "Text :  right\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En67NB6_vX-E"
      },
      "source": [
        "### 드디어 내 목소리로 Predict해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7UCRk4LvoG3"
      },
      "source": [
        "audacity로 1초동안 8000hz로 **down**과 **yes**를 녹음해서 wav파일로 저장했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DPVU1fB_lEmN",
        "outputId": "14a2f78f-b377-489b-faf1-0e75c6d90c12"
      },
      "source": [
        "samples, sample_rate=librosa.load(PROJECT_PATH+'/myvoice_down_1sec.wav', sr=16000)\r\n",
        "samples=librosa.resample(samples, sample_rate, 8000)\r\n",
        "ipd.Audio(samples, rate=8000)\r\n",
        "predict(samples)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'down'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fpPySEhmp0nD",
        "outputId": "18ddf36e-23fa-4135-aba7-c40d3bd559a1"
      },
      "source": [
        "samples, sample_rate=librosa.load(PROJECT_PATH+'/myvoice_yes_1sec.wav', sr=8000)\r\n",
        "ipd.Audio(samples, rate=8000)\r\n",
        "predict(samples)\r\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'yes'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOVlKjTnwHW8"
      },
      "source": [
        "나의 목소리를 통해서도 잘 prediction 된다!!"
      ]
    }
  ]
}